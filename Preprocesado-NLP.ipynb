{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U2BHQKJL2qm"
   },
   "source": [
    "## 2. Preprocesado NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12755,
     "status": "ok",
     "timestamp": 1691058049063,
     "user": {
      "displayName": "Victor Rey García",
      "userId": "10394014121316921241"
     },
     "user_tz": -120
    },
    "id": "fCqMTlEwMEfx",
    "outputId": "8b3fec97-9491-4faf-9720-766eea42c752"
   },
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos el dataframe\n",
    "# Introducir en path_file la ruta donde hemos guardado el dataset de trabajo\n",
    "\n",
    "path_file = \"XXXXX/df_videogames.csv\" \n",
    "df_videogames = pd.read_csv(path_file, sep=',')\n",
    "\n",
    "# Importamos librerías\n",
    "\n",
    "!pip install stop-words\n",
    "!pip install unicodedata2\n",
    "!pip install regex\n",
    "from stop_words import get_stop_words\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Relizaremos un primer preprocesado que elimine todo carácter que no sea una letra, creamos para ello una función\n",
    "\n",
    "def review_normalization(review):\n",
    "    review = unicodedata.normalize('NFKD', review).lower().encode('ascii', errors='ignore').decode('utf-8')\n",
    "    review = re.sub(' +', ' ', ' '.join([word if word.isalpha() else '' for word in review.split()])).strip()\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3928,
     "status": "ok",
     "timestamp": 1691058059776,
     "user": {
      "displayName": "Victor Rey García",
      "userId": "10394014121316921241"
     },
     "user_tz": -120
    },
    "id": "6FyeGmoqU5CL",
    "outputId": "6e6bcf4f-11b7-4ea5-892f-596b7e7f16b2"
   },
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "\n",
    "!pip install gensim\n",
    "import gensim\n",
    "\n",
    "# Realizaremos un segundo nivel de preprocesado eliminando algunas stopwords adicionales (las palabras derivadas de 'game'), creamos para ello otra función\n",
    "# Aplicaremos en este segundo nivel de preprocesado un filtrado adicional de las stopwords de gensim\n",
    "\n",
    "sw_list = get_stop_words('en')\n",
    "sw_list.append(['game', 'games', 'game.'])\n",
    "\n",
    "def remove_stopwords(review, sw_list):\n",
    "    review = ' '.join([word for word in review.split() if (word not in sw_list) or (word not in gensim.parsing.preprocessing.STOPWORDS)])\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 4182,
     "status": "ok",
     "timestamp": 1691058071410,
     "user": {
      "displayName": "Victor Rey García",
      "userId": "10394014121316921241"
     },
     "user_tz": -120
    },
    "id": "PfSLNpb0ckaA",
    "outputId": "e989d1b3-c0bf-4e00-e83e-cef70549c6d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'house'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos librerías\n",
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Realizaremos un tercer y último nivel de preprocesado en el que aplicaremos una lematización simple, creamos para ello una nueva función\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(review):\n",
    "    review = ' '.join(lemmatizer.lemmatize(word).lower().strip() for word in review.split())\n",
    "    return review\n",
    "\n",
    "# Comprobamos, con un ejemplo sencillo, que la lematización funciona correctamente\n",
    "\n",
    "lemmatizer.lemmatize('houses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1691058074083,
     "user": {
      "displayName": "Victor Rey García",
      "userId": "10394014121316921241"
     },
     "user_tz": -120
    },
    "id": "vN2NM8qnZvkG"
   },
   "outputs": [],
   "source": [
    "# Creamos la función base para realizar el preprocesado completo (que ejecutará las 3 funciones creadas anteriormente)\n",
    "\n",
    "def process_reviews(reviews, sw_list):\n",
    "    processed_reviews = []\n",
    "\n",
    "    for review in reviews:\n",
    "            review = review_normalization(review)\n",
    "            review = remove_stopwords(review, sw_list)\n",
    "            review = lemmatization(review)\n",
    "            processed_reviews.append(review)\n",
    "    return processed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 21532,
     "status": "ok",
     "timestamp": 1691058097739,
     "user": {
      "displayName": "Victor Rey García",
      "userId": "10394014121316921241"
     },
     "user_tz": -120
    },
    "id": "F3wzFvPpbUsO"
   },
   "outputs": [],
   "source": [
    "# Ejecutamos el preprocesado completo\n",
    "\n",
    "processed_reviews = process_reviews(df_videogames['reviewText'], sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1691058100983,
     "user": {
      "displayName": "Victor Rey García",
      "userId": "10394014121316921241"
     },
     "user_tz": -120
    },
    "id": "DxwjqaOpjWMg",
    "outputId": "bac3eac6-41f0-4572-85a3-23e695ed191e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review original: This game could have been so much more.  The hack and slash action is not even fun, it just get's tedious and boring.  I have to admit the graphic's are spectacular, but that does not make this a great game.  Rune contains lot's of jumping from object to object, which will get boring after they first introduce it into the game.  The game also had no depth whatsoever.  I never just want to play it for fun, because I felt there almost is no fun in this game.  In my opinion, if you are going to get a hack and slash action game GET BLADE OF DARKNESS.\n",
      "Review procesada: game much hack slash action even just tedious admit make great rune contains jumping object will get boring first introduce game also depth never just want play felt almost fun going get hack slash action game get blade\n"
     ]
    }
   ],
   "source": [
    "# Observamos cómo ha funcionado el preprocesado\n",
    "\n",
    "print('Review original: {}'.format(df_videogames['reviewText'].values[0]))\n",
    "print('Review procesada: {}'.format(processed_reviews[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1691058108746,
     "user": {
      "displayName": "Victor Rey García",
      "userId": "10394014121316921241"
     },
     "user_tz": -120
    },
    "id": "6Alz2H65l8TS"
   },
   "outputs": [],
   "source": [
    "# Cargamos la reviews preprocesadas en el dataframe\n",
    "\n",
    "df_videogames.loc[:,'processedreviews'] = processed_reviews\n",
    "\n",
    "# Eliminamos posibles valores nulos derivados del preprocesado\n",
    "\n",
    "df_videogames['processedreviews'] = df_videogames['processedreviews'].replace('', np.nan)\n",
    "df_videogames = df_videogames.dropna()\n",
    "\n",
    "# Eliminamos la variable 'reviewText', que ya no utilizaremos de cara al análisis\n",
    "\n",
    "df_videogames = df_videogames.drop('reviewText', axis = 1)\n",
    "\n",
    "# Llegados a este punto consideramos que el dataframe ya está preprocesado y listo para el análisis, lo guardamos en formato csv para posteriores usos\n",
    "    #df_videogames.to_csv('XXXXX/df_videogames_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4gxIeS4pXb-"
   },
   "source": [
    "# Conclusiones\n",
    "\n",
    "Como ya se observó en la fase anterior, el preprocesado, eliminando signos de puntuación y stopwords, ha dejado un corpus limpio, sin \"ruido\" y listo para el análisis. En este sentido se ha comprobado que es muy sencillo aplicar una doble eliminación de stopwords (añadimos la de gensim), así como una lematización, lo que supondrá una mayor limpieza y menor complejidad del corpus de cara a un análisis más eficiente."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVzZTqABL5lgfChw49n3Se",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
